{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction to NLTK\n",
    "\n",
    "We have seen how to do [some basic text processing in Python](https://github.com/Mashimo/datascience/blob/master/03-NLP/helloworld-nlp.ipynb), now we introduce an open source framework for natural language processing that can further help to work with human languages: [NLTK (Natural Language ToolKit)](http://www.nltk.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple text in a Python string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleText1 = \"The Elephant's 4 legs: THE Pub! You can't believe it or can you, the believer?\"\n",
    "sampleText2 = \"Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic atomic part of each text are the tokens. A token is the NLP name for a sequence of characters that we want to treat as a group.\n",
    "We have seen how we can extract tokens by splitting the text at the blank spaces.  \n",
    "NTLK has a function word_tokenize() for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Elephant',\n",
       " \"'s\",\n",
       " '4',\n",
       " 'legs',\n",
       " ':',\n",
       " 'THE',\n",
       " 'Pub',\n",
       " '!',\n",
       " 'You',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'believe',\n",
       " 'it',\n",
       " 'or',\n",
       " 'can',\n",
       " 'you',\n",
       " ',',\n",
       " 'the',\n",
       " 'believer',\n",
       " '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1Tokens = nltk.word_tokenize(sampleText1)\n",
    "s1Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s1Tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21 tokens extracted, which include words and punctuation.  \n",
    "Note that the tokens are different than what a split by blank spaces would obtained, e.g. \"can't\" is by NTLK considered TWO tokens: \"can\" and \"n't\" (= \"not\") while a tokeniser that splits text by spaces would consider it a single token: \"can't\".  \n",
    "Let's see another example:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2Tokens = nltk.word_tokenize(sampleText2)\n",
    "s2Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can apply it to an entire book, \"The Prince\" by Machiavelli that we used last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to work with the raw text you can use 'bookRaw'\n",
    "with open('../datasets/ThePrince.txt', 'r') as f:\n",
    "    bookRaw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bookTokens = nltk.word_tokenize(bookRaw)\n",
    "bookText = nltk.Text(bookTokens) # special format\n",
    "nBookTokens= len(bookTokens) # or alternatively len(bookText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Analysing book ***\n",
      "The book is 300814 chars long\n",
      "The book has 59792 tokens\n"
     ]
    }
   ],
   "source": [
    "print (\"*** Analysing book ***\")    \n",
    "print (\"The book is {} chars long\".format (len(bookRaw)))\n",
    "print (\"The book has {} tokens\".format (nBookTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the NTLK tokeniser works in a more sophisticated way than just splitting by spaces, therefore we got this time more tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences\n",
    "\n",
    "NTLK has a function to tokenise a text not in words but in sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"This is the first sentence. A liter of milk in the U.S. costs $0.99. Is this the third sentence? Yes, it is!\"\n",
    "sentences = nltk.sent_tokenize(text1)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A liter of milk in the U.S. costs $0.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, it is not splitting just after each full stop but check if it's part of an acronym (U.S.) or a number (0.99).  \n",
    "It also splits correctly sentences after question or exclamation marks but not after commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book has 1416 sentences\n",
      "and each sentence has in average 42.22598870056497 tokens\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(bookRaw) # extract sentences\n",
    "nSent = len(sentences)\n",
    "print (\"The book has {} sentences\".format (nSent))\n",
    "print (\"and each sentence has in average {} tokens\".format (nBookTokens / nSent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common tokens\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "The NTLK FreqDist class is used to encode “frequency distributions”, which count the number of times that something occurs, for example a token.\n",
    "\n",
    "Its `most_common()` method then returns a list of tuples where each tuple is of the form `(token, frequency)`. The list is sorted in descending order of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_words(tokens):\n",
    "        # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    return fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4192),\n",
       " ('the', 2954),\n",
       " ('to', 2081),\n",
       " ('and', 1794),\n",
       " ('of', 1772),\n",
       " ('.', 1397),\n",
       " ('in', 946),\n",
       " ('he', 844),\n",
       " ('a', 759),\n",
       " ('that', 735),\n",
       " ('his', 631),\n",
       " ('not', 562),\n",
       " ('it', 537),\n",
       " (';', 531),\n",
       " ('by', 495),\n",
       " ('with', 491),\n",
       " ('be', 467),\n",
       " ('is', 436),\n",
       " ('they', 422),\n",
       " ('him', 416)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topBook = get_top_words(bookTokens)\n",
    "  # Output top 20 words\n",
    "topBook[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comma is the most common: we need to remove the punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common alphanumeric tokens\n",
    "\n",
    "We can use `isalpha()` to check if the token is a word and not punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2954, 'the'),\n",
       " (2081, 'to'),\n",
       " (1794, 'and'),\n",
       " (1772, 'of'),\n",
       " (946, 'in'),\n",
       " (844, 'he'),\n",
       " (759, 'a'),\n",
       " (735, 'that'),\n",
       " (631, 'his'),\n",
       " (562, 'not'),\n",
       " (537, 'it'),\n",
       " (495, 'by'),\n",
       " (491, 'with'),\n",
       " (467, 'be'),\n",
       " (436, 'is'),\n",
       " (422, 'they'),\n",
       " (416, 'him'),\n",
       " (409, 'for')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topWords = [(freq, word) for (word,freq) in topBook if word.isalpha() and freq > 400]\n",
    "topWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove any capital letters before tokenising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessText(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    return [word for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bookWords = preprocessText(bookRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3110),\n",
       " ('to', 2108),\n",
       " ('and', 1938),\n",
       " ('of', 1802),\n",
       " ('in', 993),\n",
       " ('he', 921),\n",
       " ('a', 781),\n",
       " ('that', 745),\n",
       " ('his', 640),\n",
       " ('it', 586),\n",
       " ('not', 566),\n",
       " ('by', 506),\n",
       " ('with', 497),\n",
       " ('be', 471),\n",
       " ('for', 443),\n",
       " ('they', 442),\n",
       " ('is', 438),\n",
       " ('him', 417),\n",
       " ('have', 390),\n",
       " ('was', 380)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topBook = get_top_words(bookWords)\n",
    "# Output top 20 words\n",
    "topBook[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Analysing book ***\n",
      "The text has now 52202 words (tokens)\n"
     ]
    }
   ],
   "source": [
    "print (\"*** Analysing book ***\")    \n",
    "print (\"The text has now {} words (tokens)\".format (len(bookWords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we removed the punctuation and the capital letters but the most common token is \"the\", not a significative word ...  \n",
    "As we have seen last time, these are so-called **stop words** that are very common and are normally stripped from a text when doing these kind of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaningful most common tokens\n",
    "\n",
    "A simple approach could be to filter the tokens that have a length greater than 5 and frequency of more than 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['against',\n",
       " 'always',\n",
       " 'because',\n",
       " 'castruccio',\n",
       " 'having',\n",
       " 'himself',\n",
       " 'others',\n",
       " 'people',\n",
       " 'prince',\n",
       " 'project',\n",
       " 'should',\n",
       " 'therefore']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meaningfulWords = [word for (word,freq) in topBook if len(word) > 5 and freq > 80]\n",
    "sorted(meaningfulWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would work but would leave out also tokens such as `I` and `you` which are actually significative.  \n",
    "The better approach - that we have seen earlier how - is to remove stopwords using external files containing the stop words.  \n",
    "NLTK has a corpus of stop words in several languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwordsEN = set(stopwords.words('english')) # english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "betterWords = [w for w in bookWords if w not in stopwordsEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 302),\n",
       " ('prince', 222),\n",
       " ('would', 165),\n",
       " ('men', 163),\n",
       " ('castruccio', 142),\n",
       " ('people', 116),\n",
       " ('may', 110),\n",
       " ('many', 101),\n",
       " ('others', 96),\n",
       " ('time', 95),\n",
       " ('ought', 94),\n",
       " ('therefore', 92),\n",
       " ('duke', 91),\n",
       " ('great', 89),\n",
       " ('project', 87),\n",
       " ('state', 86),\n",
       " ('always', 81),\n",
       " ('man', 80),\n",
       " ('without', 79),\n",
       " ('new', 75)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topBook = get_top_words(betterWords)\n",
    "# Output top 20 words\n",
    "topBook[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we excluded words such as `the` but we can improve further the list by looking at semantically similar words, such as plural and singular versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'princes' in betterWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betterWords.count(\"prince\") + betterWords.count(\"princes\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, in the list of words we have both `prince` and `princes` which are respectively the singular and plural version of the same word (the **stem**). The same would happen with verb conjugation (`love` and `loving` are considered different words but are actually *inflections* of the same verb).  \n",
    "**Stemmer** is the tool that reduces such inflectional forms into their stem, base or root form and NLTK has several of them (each with a different heuristic algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'lists', 'listing', 'listings']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we apply one of the NLTK stemmer, the Porter stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, all 5 different words have been reduced to the same stem and would be now the same lexical token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 316),\n",
       " ('princ', 281),\n",
       " ('would', 165),\n",
       " ('men', 163),\n",
       " ('castruccio', 142),\n",
       " ('state', 137),\n",
       " ('time', 129),\n",
       " ('peopl', 118),\n",
       " ('may', 110),\n",
       " ('work', 108),\n",
       " ('great', 106),\n",
       " ('mani', 101),\n",
       " ('other', 96),\n",
       " ('ought', 94),\n",
       " ('duke', 92),\n",
       " ('therefor', 92),\n",
       " ('arm', 92),\n",
       " ('make', 90),\n",
       " ('project', 87),\n",
       " ('wish', 83)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmedWords = [porter.stem(w) for w in betterWords]\n",
    "topBook = get_top_words(stemmedWords)\n",
    "topBook[:20]  # Output top 20 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the word `princ` is counted 281 times, exactly like the sum of prince and princes.  \n",
    "\n",
    "A note here: Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.  \n",
    "`Prince` and `princes` become `princ`.  \n",
    "A different flavour is the **lemmatisation** that we will see in one second, but first a note about stemming in other languages than English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming in other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Snowball`** is an improvement created by Porter: a language to create stemmers and have rules for many more languages than English. \n",
    "For example Italian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmerIT = SnowballStemmer(\"italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputIT = \"Io ho tre mele gialle, tu hai una mela gialla e due pere verdi\"\n",
    "wordsIT = inputIT.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['io',\n",
       " 'ho',\n",
       " 'tre',\n",
       " 'mel',\n",
       " 'gialle,',\n",
       " 'tu',\n",
       " 'hai',\n",
       " 'una',\n",
       " 'mel',\n",
       " 'giall',\n",
       " 'e',\n",
       " 'due',\n",
       " 'per',\n",
       " 'verd']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmerIT.stem(w) for w in wordsIT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the **base or dictionary form of a word, which is known as the lemma**.  \n",
    "While a stemmer operates on a single word without knowledge of the context, a lemmatiser can take the context in consideration.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has also a built-in lemmatiser, so let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'lists', 'listing', 'listings']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'list', 'listing', 'listing']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(w, 'n') for w in words1] # n = nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tell the lemmatise that the words are nouns. In this case it considers the same lemma words such as list (singular noun) and lists (plural noun) but leave as they are the other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(w, 'v') for w in words1] # v = verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a different result if we say that the words are verbs.  \n",
    "They have all the same lemma, in fact they could be all different inflections or conjugation of a verb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of words that can be used are:  \n",
    "'n' = noun, 'v'=verb, 'a'=adjective, 'r'=adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words2 = ['good', 'better']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'better']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(w) for w in words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'good']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(w, 'a') for w in words2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works with different adjectives, it doesn't look only at prefixes and suffixes.  \n",
    "You would wonder why stemmers are used, instead of always using lemmatisers: stemmers are much simpler, smaller and faster and for many applications good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we lemmatise the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 316),\n",
       " ('prince', 281),\n",
       " ('would', 165),\n",
       " ('men', 163),\n",
       " ('castruccio', 142),\n",
       " ('state', 130),\n",
       " ('time', 129),\n",
       " ('people', 118),\n",
       " ('may', 110),\n",
       " ('work', 103),\n",
       " ('many', 101),\n",
       " ('others', 96),\n",
       " ('ought', 94),\n",
       " ('duke', 92),\n",
       " ('therefore', 92),\n",
       " ('great', 89),\n",
       " ('project', 87),\n",
       " ('way', 83),\n",
       " ('make', 81),\n",
       " ('always', 81)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatisedWords = [lemmatizer.lemmatize(w, 'n') for w in betterWords]\n",
    "topBook = get_top_words(lemmatisedWords)\n",
    "topBook[:20]  # Output top 20 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the lemma now is `prince`.  \n",
    "But note that we consider all words in the book as nouns, while actually a proper way would be to apply the correct type to each single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech (PoS)\n",
    "\n",
    "In traditional grammar, a part of speech (abbreviated form: PoS or POS) is a category of words which have similar grammatical properties. \n",
    "\n",
    "For example, an adjective (red, big, quiet, ...) describe properties while a verb (throw, walk, have) describe actions or states.\n",
    "\n",
    "Commonly listed parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "tokensT1 = nltk.word_tokenize(text1)\n",
    "nltk.pos_tag(tokensT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK function `pos_tag()` will tag each token with the estimated PoS.  \n",
    "NLTK has 13 categories of PoS. You can check the acronym using the NLTK help function:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the most common PoS in The Prince book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IN', 7218), ('NN', 5992), ('DT', 5374), (',', 4192), ('PRP', 3489)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokensAndPos = nltk.pos_tag(bookTokens)\n",
    "posList = [thePOS for (word, thePOS) in tokensAndPos]\n",
    "fdistPos = nltk.FreqDist(posList)\n",
    "fdistPos.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('IN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not nouns (NN) but interections (IN) such as preposition or conjunction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra note: Parsing the grammar structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words can be ambiguous and sometimes is not easy to understand which kind of POS is a word, for example in the sentence \"visiting aunts can be a nuisance\", is visiting a verb or an adjective?  \n",
    "Tagging a PoS depends on the context, which can be ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sense of a sentence is easier if it follows a well-defined grammatical structure, such as : subject + verb + object  \n",
    "NLTK allows to define a formal grammar which can then be used to parse a text.  The NLTK ChartParser is a procedure for finding one or more trees (sentences have internal organisation that can be represented using a tree) corresponding to a grammatically well-formed sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "# Parsing sentence structure\n",
    "text2 = nltk.word_tokenize(\"Alice loves Bob\")\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "NP -> 'Alice' | 'Bob'\n",
    "V -> 'loves'\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(text2)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a \"toy grammar,\" a small grammar that illustrate the key aspects of parsing. But there is an obvious question as to whether the approach can be scaled up to cover large corpora of natural languages. How hard would it be to construct such a set of productions by hand? In general, the answer is: very hard.  \n",
    "Nevertheless, there are efforts to develop broad-coverage grammars, such as weighted and probabilistic grammars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The world outside NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As a final note, NLTK was used here for educational purpose but you should be aware that has its own limitations.  \n",
    "NLTK is a solid library but it's old and slow. Especially the NLTK's lemmatisation functionality is slow enough that it will become the bottleneck in almost any application that will use it.\n",
    "\n",
    "For industrial NLP application a very performance-minded Python library is  [SpaCy.io](https://spacy.io/) instead.  \n",
    "And for robust multi-lingual support there is [polyglot](http://polyglot.readthedocs.io/en/latest/) that has a much wider language support of all the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other tools exist in other computer languages such as Stanford CoreNLP and Apache OpenNLP, both in Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
